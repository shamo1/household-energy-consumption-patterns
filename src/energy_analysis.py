# -*- coding: utf-8 -*-
"""hashaam_2212172.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fufKc-Q-5RVW19ZHe3OF_LpQek4d__AA

# Household energy consumption patterns andbehaviours
## Importing required libraries
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px

from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.stattools import adfuller
from sklearn.feature_selection import RFE
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout

from keras.models import Sequential
from keras.regularizers import l2
from keras.callbacks import EarlyStopping, ReduceLROnPlateau
from keras.metrics import MeanSquaredError, MeanAbsoluteError

from statsmodels.tsa.arima.model import ARIMA
from kerastuner import HyperModel
from kerastuner.tuners import RandomSearch
from prophet import Prophet
from prophet.diagnostics import cross_validation, performance_metrics

# Keras tunner install
# !pip install keras-tuner

path = "/content/drive/MyDrive/Disssertation MA891/household_data_15min_singleindex.csv"
df = pd.read_csv(path)

df

"""## Exploring dataset"""

df.info()

df.isnull().sum()

df.describe()

missing_values = df.isnull()
plt.figure(figsize=(10, 6))
sns.heatmap(missing_values, cbar=False, cmap='viridis')
plt.title('Missing Values Heatmap')
plt.show()

# Filtering out each target dataframe into new dataset e.g residential4 with all the predictors and timestamp
def filtered_Dataset(df,  col_name, index):
    selected_columns = [col for col in df.columns if col.startswith(col_name + index)]
    selected_columns = ['utc_timestamp']+selected_columns
    return df[selected_columns]

import os
path = '/content/drive/MyDrive/Disssertation 981/Open Power System (Code)'
if os.path.exists:
    output_path = os.path.join(path,'residential4.csv')
    filtered_Dataset(df, 'DE_KN_residential', '4').to_csv(output_path,index=False)
    df  =filtered_Dataset(df,'DE_KN_residential','4')
else:
    os.mkdir(path=path)
    output_path = os.path.join(path,'residential4.csv')
    filtered_Dataset(df, 'DE_KN_residential', '4').to_csv(output_path,index=False)
    df  =filtered_Dataset(df,'DE_KN_residential','4')

missing_values = df.isnull()
plt.figure(figsize=(10, 6))
sns.heatmap(missing_values, cbar=False, cmap='viridis')
plt.title('Missing Values Heatmap')
plt.show()

print(f'DataShape: {df.shape}')
print(f'Data Info: {df.info()}')
print(f'Data Missing Values: {df.isna().sum()}')

"""# Dropping the missing values at the start of the each columns"""

# Dropping all the rows with null values until start the dataset
def drop_na(dataset,start_date):
  dataset = dataset[dataset['utc_timestamp']>= start_date]
  dataset.reset_index(drop=True, inplace=True)
  return dataset

df = drop_na(df,'2015-10-14 12:00:00+00:00')
df_copy  = df
df_copy

print(df.describe())

"""## Interpolation and Extrapolation Effect on data"""

df_interpolate = df.copy()
df_extrapolate = df.copy()

df_interpolate = df_interpolate.interpolate(method='linear')

df_interpolate = df_interpolate.interpolate(method='linear')
df_interpolate.reset_index(drop=True,inplace=True)


df_interpolate['utc_timestamp'] = pd.to_datetime(df_interpolate['utc_timestamp'])
df_interpolate.set_index('utc_timestamp',inplace=True)

import matplotlib.pyplot as plt
import matplotlib.dates as mdates


plt.figure(figsize=(15, 8))

for column in df_interpolate.columns:
    plt.plot(df_interpolate.index, df_interpolate[column], label=column)

plt.gca().xaxis.set_major_locator(mdates.MonthLocator(interval=3))
plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))

plt.legend(df_interpolate.columns)
plt.xlabel('Date')
plt.ylabel('Values')
plt.title('Interpolated Data')

def extrapolate_end_of_sequence(column):

    last_valid_index = column.last_valid_index()
    if last_valid_index is not None and last_valid_index != column.index[-1]:
        known_values = column.loc[:last_valid_index][-2:]
        known_indices = known_values.index
        slope = (known_values.iat[-1] - known_values.iat[-2]) / (known_indices[-1] - known_indices[-2])

        for i in range(known_indices[-1] + 1, len(column)):
            column.iat[i] = column.iat[i-1] + slope

    return column

for col in df_extrapolate.columns:
    df_extrapolate[col] = extrapolate_end_of_sequence(df[col])

df_extrapolate

df_extrapolate['utc_timestamp'] = pd.to_datetime(df_extrapolate['utc_timestamp'])
df_extrapolate.set_index('utc_timestamp',inplace=True)

plt.figure(figsize=(15, 8))

for column in df_interpolate.columns:
    plt.plot(df_extrapolate.index, df_interpolate[column], label=column)

plt.gca().xaxis.set_major_locator(mdates.MonthLocator(interval=3))
plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))

plt.legend(df_extrapolate.columns)
plt.xlabel('Date')
plt.ylabel('Values')
plt.title('Extrapolated Data')

df_trimmed = df.iloc[135:]
df_trimmed


first_missing_index = df_trimmed.loc[81079:, 'DE_KN_residential4_grid_import'].first_valid_index()

if first_missing_index is not None:
    data_cleaned = df_trimmed.loc[:first_missing_index - 1]
else:
    data_cleaned = df_trimmed.loc[:81079]

data_cleaned.columns

df_clean = data_cleaned.ffill()
df_clean.isnull().sum()
df_clean

df_clean.rename(columns={
    '': 'utc_timestamp',
    'DE_KN_residential4_dishwasher': 'dishwasher',
    'DE_KN_residential4_ev': 'electric_vehicle',
    'DE_KN_residential4_freezer': 'freezer',
    'DE_KN_residential4_grid_export': 'export',
    'DE_KN_residential4_grid_import': 'import',
    'DE_KN_residential4_heat_pump': 'heatpump',
    'DE_KN_residential4_pv': 'pv',
    'DE_KN_residential4_refrigerator': 'refregerator',
    'DE_KN_residential4_washing_machine': 'washingmachine'
}, inplace=True)

print(df_clean.describe())

sns.set_style("whitegrid")

# Create a vertical box plot with colored boxes
plt.figure(figsize=(12, 8))
sns.boxplot(data=df_clean, orient="v", palette="Set3")
plt.title('Exploring data')
plt.xticks(rotation=90)
plt.tight_layout()

# Display the plot
plt.show()

def remove_outliers(df):
    Q1 = df.quantile(0.25)
    Q3 = df.quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    return df[~((df < lower_bound) |(df > upper_bound)).any(axis=1)]

cleaned_df = remove_outliers(df_clean)

df_clean = cleaned_df.copy()

sns.set_style("whitegrid")

plt.figure(figsize=(12, 8))
sns.boxplot(data=df_clean, orient="v", palette="Set3")
plt.title('Exploring data')
plt.xticks(rotation=90)
plt.tight_layout()

plt.show()

correlation_matrix = df_clean.corr()
correlation_with_grid_import = correlation_matrix['import'].drop('import')
correlation_with_grid_import

df_clean['utc_timestamp'] = pd.to_datetime(df_clean['utc_timestamp'])
df_clean = df_clean.set_index('utc_timestamp')

"""## EDA"""

correlation_matrix = df_clean.corr()

correlation_with_grid_import = correlation_matrix['import'].drop('import')

correlation_with_grid_import

df_clean.reset_index(inplace = True)
appliances = ['dishwasher',	'electric_vehicle',	'freezer','heatpump',	'pv',	'refregerator',	'washingmachine']
df_clean['day_of_week'] = df_clean['utc_timestamp'].dt.dayofweek

weekdays = df_clean[df_clean['day_of_week'] <= 4]
weekends = df_clean[df_clean['day_of_week'] >= 5]

total_consumption_weekdays_per_appliance = weekdays[appliances].sum()
total_consumption_weekends_per_appliance = weekends[appliances].sum()

plt.figure(figsize=(14, 8))

width = 0.35
indices = np.arange(len(appliances))

plt.bar(indices - width/2, total_consumption_weekdays_per_appliance, width, label='Weekdays', color='skyblue')
plt.bar(indices + width/2, total_consumption_weekends_per_appliance, width, label='Weekends', color='coral')

plt.xlabel('Appliances')
plt.ylabel('Total Energy Consumption (kWh)')
plt.title('Total Energy Consumption per Appliance: Weekdays vs Weekends')
plt.xticks(indices, [appliance.split('_')[-1] for appliance in appliances], rotation=45)
plt.legend()
plt.grid(True)

plt.show()

df_clean['utc_timestamp'] = pd.to_datetime(df_clean['utc_timestamp'])
df_clean['day_of_week'] = df_clean['utc_timestamp'].dt.dayofweek  # Monday=0, Sunday=6
df_clean['hour_of_day'] = df_clean['utc_timestamp'].dt.hour

df_clean['total_energy_consumption'] = df_clean[appliances].sum(axis=1)

percent_df = pd.DataFrame(index=df_clean.index)

for appliance in appliances:
    percent_df[f'{appliance}_percent'] = 100 * df_clean[appliance] / df_clean['total_energy_consumption']

average_percent_contribution_by_hour = percent_df.groupby(df_clean['hour_of_day']).mean()

plt.figure(figsize=(15, 8))

sns.set_style("whitegrid")

plt.stackplot(average_percent_contribution_by_hour.index, average_percent_contribution_by_hour.T, labels=appliances, alpha=0.8)

plt.title("Average Percentage Contribution of Each Appliance to Total Energy Consumption by Hour of Day", fontsize=16)
plt.xlabel("Hour of Day", fontsize=14)
plt.ylabel("Percentage Contribution (%)", fontsize=14)

plt.legend(title="Appliances", title_fontsize='13', fontsize='12', loc='upper left')

plt.xticks(fontsize=12)
plt.yticks(fontsize=12)

plt.grid(True, which='both', linestyle='--', linewidth=0.5)

plt.show()

df_clean['utc_timestamp'] = pd.to_datetime(df['utc_timestamp'])

df_clean.set_index('utc_timestamp',inplace = True)

plt.figure(figsize=(15, 10))

plt.plot(df_clean['total_energy_consumption'], label='Total Energy Consumption', color='black', alpha=0.7)

colors = plt.cm.viridis(np.linspace(0, 1, len(appliances)))
for appliance, color in zip(appliances, colors):
    plt.plot(df_clean[appliance], label=appliance, alpha=0.7, color=color)

plt.title('Energy Usage throughtout time')
plt.ylabel('Energy Usage')
plt.xlabel('Time')

plt.legend()

plt.tight_layout()
plt.show()

max_energy_by_season = df_clean.groupby('Season')['total_energy_consumption'].max()

sns.set_style("whitegrid")
plt.figure(figsize=(10, 6))

barplot = sns.barplot(x=max_energy_by_season.index, y=max_energy_by_season.values, palette='muted')

for bar in barplot.patches:
    height_in_mWh = bar.get_height() / 1000
    barplot.annotate(f"{height_in_mWh:.2f} MWh",
                     (bar.get_x() + bar.get_width() / 2., bar.get_height()),
                     ha='center', va='center',
                     size=9, xytext=(0, 8),
                     textcoords='offset points')

plt.title('Maximum Energy consumption in different seasons')
plt.xlabel('Season')
plt.ylabel('Maximum Energy Consumption (kWh)')

plt.show()

df_clean['years'] = df_clean.index.year

seasonal_df = df_clean.pivot_table(index='years', columns='Season', values='pv', aggfunc='mean')



fig, ax = plt.subplots(figsize=(10, 6))
bars = seasonal_df.plot(kind='bar', stacked=True, ax=ax)

plt.title('Solar Generation by Year and Season')
plt.xlabel('Year')
conversion_factor = 1000
current_ticks = ax.get_yticks()
ax.set_yticks(current_ticks)
ax.set_yticklabels([f"{y / conversion_factor:.2f}" for y in current_ticks])
plt.ylabel('Solar Generation')
plt.xticks(rotation=0)

legend_labels = seasonal_df.columns.tolist()
ax.legend(legend_labels, title='Season', bbox_to_anchor=(1.05, 1), loc='upper left')

overall_yearly_mean = df_clean['pv'].mean()
plt.ylabel(f'Solar Generation\n(Overall Mean: {overall_yearly_mean:.2f} kWh)')

for bar in bars.patches:
    height = bar.get_height()
    if height > 0:
        height_in_mWh = height / conversion_factor
        ax.text(bar.get_x() + bar.get_width() / 2,
                bar.get_y() + height / 2,
                f'{height_in_mWh:.2f} Mwh',
                ha='center',
                va='center',
                rotation=0,
                color='white',
                fontsize=8)

plt.tight_layout()
plt.show()

#df_clean_clean['utc_timestamp'] = pd.to_datetime(df_clean['utc_timestamp'])
#df_clean['years'] = df_clean.index.year

df_clean['energy_saving'] = df_clean['export'] - df_clean['import']

yearly_energy_saving = df_clean.groupby('years')['energy_saving'].mean()


yearly_energy_saving_mwh = yearly_energy_saving / 1000

plt.figure(figsize=(12, 6))
barplot = sns.barplot(x=yearly_energy_saving_mwh.index, y=yearly_energy_saving_mwh.values, palette="tab10")
plt.title('Energy saved on yearly basis', fontsize=16)
plt.xlabel('Year', fontsize=14)
plt.ylabel('Energy Saving (MWh)', fontsize=14)
plt.xticks(rotation=45)
plt.grid(axis='y')

for p in barplot.patches:
    if p.get_height() > 0:
        barplot.annotate(f"{p.get_height():.2f} MWh",
                         (p.get_x() + p.get_width() / 2., p.get_height()),
                         ha = 'center', va = 'center',
                         xytext = (0, 9),
                         textcoords = 'offset points')

plt.tight_layout()
plt.show()

seasonal_data = df_clean.groupby('Season')[appliances + ['pv']].mean()
seasonal_device_consumption = seasonal_data[appliances]
seasonal_device_consumption



seasonal_device_consumption = seasonal_data[appliances]
seasonal_solar_generation = seasonal_data[['pv']]



seasonal_solar_generation_long = seasonal_data[['pv']].reset_index().melt(id_vars='Season', var_name='Type', value_name='Average Generation')

plt.figure(figsize=(10, 6))
sns.barplot(x='Season', y='Average Generation', data=seasonal_solar_generation_long)
plt.title('Average Solar Energy Generation Across Seasons')
plt.xlabel('Season')
plt.ylabel('Average Solar Generation')
plt.xticks(rotation=45)
plt.tight_layout()

plt.show()

mAppliances = ['dishwasher', 'refregerator', 'washingmachine','electric_vehicle','heatpump']

daily_consumption = df_clean[mAppliances + ['total_energy_consumption']].resample('D').sum()

for appliance in mAppliances:
    daily_consumption[f'{appliance}_prop'] = daily_consumption[appliance] / daily_consumption['total_energy_consumption']

plt.figure(figsize=(15, 7))
plt.stackplot(daily_consumption.index,
              [daily_consumption[f'{appliance}_prop'] for appliance in mAppliances],
              labels=mAppliances, alpha=0.8)
plt.title("Proportion of Total Energy Consumption by Each Appliance Over Time")
plt.xlabel("Date")
plt.ylabel("Proportion of Total Consumption")
plt.legend(loc='upper left')
plt.show()

time_slots = {
    'morning': (5, 12),
    'afternoon': (12, 17),
    'evening': (17, 22),
    'night': (22, 5)
}

def energy_consumption_in_slot(df, start_hour, end_hour):
    if start_hour < end_hour:
        return df.between_time(f'{start_hour}:00', f'{end_hour}:00', include_end=False).sum()
    else:
        return df.between_time(f'{start_hour}:00', '23:59', include_end=True).sum() + df.between_time('00:00', f'{end_hour}:00', include_end=False).sum()

hourly_energy_consumption = df_clean['total_energy_consumption'].resample('H').sum()

daily_slot_consumption = pd.DataFrame(index=hourly_energy_consumption.resample('D').sum().index)


for slot, (start, end) in time_slots.items():
    daily_slot_consumption[slot] = df_clean['total_energy_consumption'].resample('D').apply(
        lambda x: energy_consumption_in_slot(x, start, end)
    )


daily_slot_consumption.columns = time_slots.keys()

scaler = StandardScaler()
scaled_features = scaler.fit_transform(daily_slot_consumption)

kmeans = KMeans(n_clusters=4, random_state=0)
clusters = kmeans.fit_predict(scaled_features)

daily_slot_consumption['Cluster'] = clusters
daily_slot_consumption

daily_consumption = daily_slot_consumption.groupby(daily_slot_consumption.index.date).agg({'morning': 'mean', 'afternoon': 'mean', 'evening': 'mean', 'night': 'mean'})

plt.figure(figsize=(12, 8))
sns.clustermap(daily_consumption, cmap="viridis", annot=True)

plt.title("Energy Consumption Patterns During Different Periods of the Day", fontsize=16)
plt.xlabel("Period of the Day")
plt.ylabel("Date")

plt.show()

"""## Time Series decomposition"""

result_target = seasonal_decompose(df_clean['total_energy_consumption'], model='additive', period=17520)

trend = result_target.trend.dropna()
seasonal = result_target.seasonal.dropna()
residual = result_target.resid.dropna()

plt.figure(figsize=(14, 10))

plt.subplot(311)
plt.plot(trend, label='Trend', color='blue')
plt.legend(loc='best')
plt.title('Trend Component of Energy Consumption')
plt.tight_layout()

plt.subplot(312)
plt.plot(seasonal, label='Seasonal', color='orange')
plt.legend(loc='best')
plt.title('Seasonal Component of Energy Consumption')
plt.tight_layout()

plt.subplot(313)
plt.plot(residual, label='Residual', color='green')
plt.legend(loc='best')
plt.title('Residual Component of Energy Consumption')
plt.tight_layout()

plt.tight_layout(pad=3.0)

plt.show()

"""## Stishnary Check"""

time_series = df_clean['total_energy_consumption'].fillna(method='ffill')

adf_test_result = adfuller(time_series.dropna())

adf_test_statistic, adf_p_value = adf_test_result[0], adf_test_result[1]

print(f"ADF Test Statistics: {adf_test_statistic}")
print(f"ADF p-values: {adf_p_value}")

df_clean = df_clean.drop(['years','energy_saving'],axis=1)

# Correlation
correlation_matrix = df_clean.corr()

plt.figure(figsize=(14, 10))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5, fmt=".2f")
plt.title('Correlation Heatmap')
plt.tight_layout()
plt.show()

"""## Feature Enginering


"""

target_corr = df_clean.corrwith(df_clean['total_energy_consumption'])

# Convert the Series to a DataFrame
corr_df = pd.DataFrame(target_corr, columns=['Correlation'])

# Resetting the index to get feature names as a column
corr_df.reset_index(inplace=True)
corr_df.rename(columns={'index': 'Feature'}, inplace=True)

# Plotting the correlation values
plt.figure(figsize=(10, 8))
sns.barplot(x='Correlation', y='Feature', data=corr_df)
plt.title('Correlation with Target Variable')
plt.show()

df_clean = df_clean.drop('Season',axis=1)

X = df_clean.drop(['total_energy_consumption'], axis=1)
y = df_clean['total_energy_consumption']

model = LinearRegression()
rfe = RFE(estimator=model, n_features_to_select=None)
rfe.fit(X, y)

selected_features = X.columns[rfe.support_]

feature_importance = rfe.estimator_.coef_

sorted_idx = np.argsort(feature_importance)
sorted_features = selected_features[sorted_idx]
sorted_importance = feature_importance[sorted_idx]

plt.figure(figsize=(12, 6))
plt.title("Feature Importance for Total Energy Consumption Prediction")
plt.barh(range(len(sorted_features)), sorted_importance, align='center')
plt.yticks(range(len(sorted_features)), sorted_features)
plt.xlabel('Coefficient Value')
plt.ylabel('Features')
plt.tight_layout()
plt.show()

X = df_clean.drop(['total_energy_consumption'], axis=1)
y = df_clean['total_energy_consumption']

rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)
rf_regressor.fit(X, y)

importances = rf_regressor.feature_importances_

indices = np.argsort(importances)[::-1]

feature_names = X.columns[indices]

plt.figure(figsize=(12, 6))
plt.title("Feature Importance for Total Energy Consumption Prediction")

# Reversing the order of features and importances
reversed_indices = indices[::-1]
reversed_feature_names = feature_names[reversed_indices]

plt.barh(range(X.shape[1]), importances[reversed_indices], align='center')
plt.yticks(range(X.shape[1]), reversed_feature_names)
plt.xlabel('Importance')
plt.ylabel('Features')
plt.tight_layout()
plt.show()

correlations = X.corrwith(y)
features_corr = correlations[correlations.abs() > 0.1].index.tolist()
print(f"Selected features using Correlation Analysis: {features_corr}")
print(f"Selected features using Random Forest: {feature_names}")
print(f"Selected features using Recursive Feature Enginering: {selected_features}")

common_features =  set(features_corr) & set(feature_names) & set(selected_features)

df_cleaned = df_clean.reset_index()

columnslist =  ['utc_timestamp'] + ['total_energy_consumption'] + list(common_features)[:3]


df_common_features = df_cleaned[columnslist]
fb_common_features = df_common_features.copy()
lstm_common_features = df_common_features.copy()
arima_common_features = df_common_features.copy()
lr_common_features = df_common_features.copy()
gb_common_features = df_common_features.copy()
rf_common_features = df_common_features.copy()

df_common_features

"""# Predicting total energy consumption"""

df_common_feature_lr =df_common_features.copy()
df_common_feature_lr = df_common_feature_lr.drop('utc_timestamp',axis=1)
X = df_common_feature_lr.drop('total_energy_consumption', axis=1)
y = df_common_feature_lr['total_energy_consumption']

lr_X_train, ls_X_test, lr_y_train, lr_y_test = train_test_split(X, y, test_size=0.2, random_state=0)

model = LinearRegression()

model.fit(lr_X_train, lr_y_train)

lr_y_pred = model.predict(ls_X_test)

mse_reg = mean_squared_error(lr_y_test, lr_y_pred)
rmse_reg = np.sqrt(mse_reg)
r2 = r2_score(lr_y_test, lr_y_pred)

print(f"Linear Regression MSE:{mse_reg}")
print(f"RMSE: {rmse_reg}")
print(f"R2: {r2}")

plt.figure(figsize=(10, 5))
plt.scatter(lr_y_test, lr_y_pred, alpha=0.5)
plt.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=2)
plt.xlabel('Actual')
plt.ylabel('Predicted')
plt.title('Validation Plot')
plt.show()



lasso_model = Lasso(alpha=0.1, random_state=0)

# Model Training
lasso_model.fit(lr_X_train, lr_y_train)

# Model Prediction
y_pred_lasso = lasso_model.predict(ls_X_test)

# Model Evaluation
mse_lasso = mean_squared_error(lr_y_test, y_pred_lasso)
r2_lasso = r2_score(lr_y_test, y_pred_lasso)

mse_lasso, r2_lasso

print(f'MSE for Lasso: {mse_lasso}')
print(f'R2 for Lasso: {r2_lasso}')

"""# LSTM"""

target_column_new = 'total_energy_consumption'
target_data_new = df_common_features[[target_column_new]]

missing_values_new = target_data_new.isnull().sum()

missing_values_new

scaler_new = MinMaxScaler(feature_range=(0, 1))
normalized_data_new = scaler_new.fit_transform(target_data_new)

normalized_data_df_new = pd.DataFrame(normalized_data_new, columns=['total_energy_consumption'])

normalized_data_df_new.head()

lstm_common_features['utc_timestamp'] = pd.to_datetime(lstm_common_features['utc_timestamp'], utc=True)
lstm_common_features.set_index('utc_timestamp', inplace=True)

time_series_lstm = lstm_common_features['total_energy_consumption']

values = time_series_lstm.values.reshape(-1, 1)

scaler = MinMaxScaler(feature_range=(0, 1))
scaled = scaler.fit_transform(values)

def cleansupervised_scaler(data, n_in=1, n_out=1, dropnan=True):

    n_vars = 1 if type(data) is list else data.shape[1]
    df = pd.DataFrame(data)
    cols, names = list(), list()
    for i in range(n_in, 0, -1):
        cols.append(df.shift(i))
        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]
    # Forecast sequence (t, t+1, ... t+n)
    for i in range(0, n_out):
        cols.append(df.shift(-i))
        if i == 0:
            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]
        else:
            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]
    # Put it all together
    agg = pd.concat(cols, axis=1)
    agg.columns = names
    # Drop rows with NaN values
    if dropnan:
        agg.dropna(inplace=True)
    return agg

reframed = cleansupervised_scaler(scaled, n_in=96, n_out=1)
reframed.drop(reframed.columns[-1], axis=1, inplace=True)
reframed

values = reframed.values
train_hours = int(len(values) * 0.8)
train = values[:train_hours, :]
test = values[train_hours:, :]

X_train, y_train = train[:, :-1], train[:, -1]
X_test, y_test = test[:, :-1], test[:, -1]

X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))
X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))

X_train.shape, y_train.shape, X_test.shape, y_test.shape

model = Sequential()
model.add(LSTM(50, input_shape=(X_train.shape[1], X_train.shape[2]),
               kernel_regularizer=l2(0.001)))
model.add(Dropout(0.4))
model.add(Dense(1))
model.compile(loss='mean_squared_error', optimizer='adam')

early_stop_lstm = EarlyStopping(monitor='val_loss', patience=10, verbose=1)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001, verbose=1)

history = model.fit(
    X_train, y_train,
    epochs=50,
    batch_size=64,
    validation_data=(X_test, y_test),
    verbose=2,
    shuffle=False,
    callbacks=[early_stop_lstm, reduce_lr] )

plt.figure(figsize=(10, 6))
plt.plot(history.history['loss'], label='Train')
plt.plot(history.history['val_loss'], label='Validation')
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend()
plt.show()

test_loss = model.evaluate(X_test, y_test, verbose=0)
print(f'Test Loss: {test_loss}')

y_pred = model.predict(X_test, verbose=0)

mse = MeanSquaredError()
mse.update_state(y_test, y_pred)
mean_squared_error = mse.result().numpy()

mae = MeanAbsoluteError()
mae.update_state(y_test, y_pred)
mean_absolute_error = mae.result().numpy()

rmse = np.sqrt(mean_squared_error)

print(f'Mean Squared Error: {mean_squared_error}')
print(f'Mean Absolute Error: {mean_absolute_error}')
print(f'Root Mean Squared Error: {rmse}')

errors = y_pred.flatten() - y_test

plt.figure(figsize=(12, 7))
sns.histplot(errors, bins=50, kde=True, color='skyblue')

plt.xlabel('Prediction Error', fontsize=14)
plt.ylabel('Frequency', fontsize=14)
plt.title('Distribution of Prediction Errors', fontsize=16)
plt.axvline(x=errors.mean(), color='r', linestyle='--', label=f'Mean error: {errors.mean():.4f}')
plt.axvline(x=np.median(errors), color='g', linestyle='-', label=f'Median error: {np.median(errors):.4f}')
plt.legend()
plt.grid(True)

plt.tight_layout()

plt.show()

"""
## ARIMA

"""

import pandas as pd

y = arima_common_features['total_energy_consumption']

arima_trian_size = int(len(y) * 0.8)
arima_trian, arima_test = y[0:arima_trian_size], y[arima_trian_size:len(y)]



# Function to arima_test stationarity
def arima_test_stationarity(timeseries):
    result = adfuller(timeseries.dropna())
    return result[1]

d = 0
temp_arima_trian = arima_trian.copy()
while arima_test_stationarity(temp_arima_trian) > 0.05:
    temp_arima_trian = temp_arima_trian.diff().dropna()
    d += 1

best_aic = np.inf
best_order = None
for p in range(4):
    for q in range(4):
        try:
            model = ARIMA(temp_arima_trian, order=(p, d, q))
            results = model.fit()
            if results.aic < best_aic:
                best_aic = results.aic
                best_order = (p, d, q)
        except:
            continue

arima_model = ARIMA(arima_trian, order=best_order)
results = arima_model.fit()

arima_prediction = results.forecast(steps=len(arima_test))
arima_mse = mean_squared_error(arima_test, arima_prediction)
arima_rmse = np.sqrt(arima_mse)

print('Mean Squared Error:', arima_mse)
print('Root Mean Squared Error:', arima_rmse)

plt.figure(figsize=(12, 6))
plt.plot(arima_trian, label='Training Data', color='blue')
plt.plot(arima_prediction, label='Forecast', color='red', alpha=0.7)

plt.title('ARIMA Model - Training Data and Forecast')
plt.xlabel('Time')
plt.ylabel('Values')
plt.legend()
plt.show()

"""## ANN"""

from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split

scaler = MinMaxScaler(feature_range=(0, 1))
scaled_features = scaler.fit_transform(df_common_features.drop(['total_energy_consumption', 'utc_timestamp'], axis=1))

y = df_common_features['total_energy_consumption'].values

X_train, X_test, y_train, y_test = train_test_split(scaled_features, y, test_size=0.3, shuffle=False)

ann_model = Sequential()
ann_model.add(Dense(60, input_shape=(X_train.shape[1], X_train.shape[2]),
               kernel_regularizer=l2(0.001)))
ann_model.add(Dropout(0.4))
ann_model.add(Dense(1))
ann_model.compile(optimizer='adam',loss='mean_squared_error')

early_stop_lstm = EarlyStopping(monitor='val_loss', patience=10, verbose=1)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001, verbose=1)

history = ann_model.fit(
    X_train, y_train,
    epochs=50,
    batch_size=64,
    validation_data=(X_test, y_test),
    verbose=2,
    shuffle=False,
    callbacks=[early_stop_lstm, reduce_lr] )

plt.figure(figsize=(10, 6))
plt.plot(history.history['loss'], label='Train')
plt.plot(history.history['val_loss'], label='Validation')
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend()
plt.show()

y_pred = ann_model.predict(X_test)
y_pred = y_pred.squeeze()
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)

print("Mean Squared Error:", mse)
print("Root Mean Squared Error:", rmse)

class ANNHyperModel(HyperModel):
    def __init__(self, input_shape):
        self.input_shape = input_shape

    def build(self, hp):
        model = Sequential()
        model.add(Dense(units=hp.Int('units', min_value=32, max_value=512, step=32),
                        activation=hp.Choice('activation', values=['relu', 'tanh']),
                        input_dim=self.input_shape))
        model.add(Dense(1))
        model.compile(
            optimizer=hp.Choice('optimizer', values=['adam', 'sgd']),
            loss='mean_squared_error'
        )
        return model

hypermodel = ANNHyperModel(input_shape=X_train.shape[1])

tuner = RandomSearch(
    hypermodel,
    objective='val_loss',
    seed=42,
    max_trials=10,
    executions_per_trial=2
)

tuner.search(X_train, y_train, epochs=10, validation_data=(X_test, y_test), verbose=2)

best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]

best_model = tuner.hypermodel.build(best_hps)
best_model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))

#df_common_features['utc_timestamp'] = df_common_features['utc_timestamp'].dt.tz_localize(None)
#df_common_features.rename(columns={'utc_timestamp': 'ds', 'total_energy_consumption': 'y'}, inplace=True)

fb_model = Prophet(
    seasonality_mode='additive',
    yearly_seasonality=True,
    weekly_seasonality=True,
    daily_seasonality=False
)

for column in df_common_features.columns:
    if column not in ['ds', 'y', 'Unnamed: 0']:
        fb_model.add_regressor(column)

fb_model.fit(df_common_features)

future = fb_model.make_future_dataframe(periods=365)
for column in df_common_features.columns:
    if column not in ['ds', 'y', 'Unnamed: 0']:
        future[column] = df_common_features[column]

forecast = fb_model.predict(future)

fig = fb_model.plot(forecast)
plt.title('Prophet Forecast for Total Energy Consumption with Regressors')
plt.xlabel('Date')
plt.ylabel('Total Energy Consumption')
plt.show()

import pandas as pd
from prophet import Prophet

"""## Fb Prophet"""

fb_common_features.rename(columns={'utc_timestamp': 'ds', 'total_energy_consumption': 'y'}, inplace=True)

fb_common_features['ds'] = pd.to_datetime(fb_common_features['ds']).dt.tz_localize(None)

fb_common_features.fillna(method='ffill', inplace=True)

fb_model = Prophet(
    seasonality_mode='additive',
    yearly_seasonality=True,
    weekly_seasonality=True,
    daily_seasonality=False
)

for column in fb_common_features.columns:
    if column not in ['ds', 'y']:
        fb_model.add_regressor(column)

fb_model.fit(fb_common_features)

future_periods = 365
future = fb_model.make_future_dataframe(periods=future_periods)

for column in fb_common_features.columns:
    if column not in ['ds', 'y']:
        last_known_value = fb_common_features[column].iloc[-1]
        future[column] = last_known_value

fb_forecast = fb_model.predict(future)

fig1 = fb_model.plot(fb_forecast)
plt.title('Prophet fb_forecast: Total Energy Consumption')
plt.xlabel('Date')
plt.ylabel('Total Energy Consumption')

fig2 = fb_model.plot_components(fb_forecast)

fb_common_features['ds'] = pd.to_datetime(fb_common_features['ds'])
comparison_df = fb_forecast.set_index('ds')[['yhat']].join(fb_common_features.set_index('ds'))
comparison_df = comparison_df.dropna()


mse = mean_squared_error(comparison_df['y'], comparison_df['yhat'])
print(f"Mean Squared Error (MSE): {mse}")

mae = mean_absolute_error(comparison_df['y'], comparison_df['yhat'])
print(f"Mean Absolute Error (MAE): {mae}")

rmse = np.sqrt(mse)
print(f"Root Mean Squared Error (RMSE): {rmse}")

cv_results = cross_validation(fb_model, horizon='100 days')

metrics = performance_metrics(cv_results)

fig = plt.figure(figsize=(10, 6))
plt.plot(metrics['horizon'], metrics['mae'], label='MAE')
plt.plot(metrics['horizon'], metrics['rmse'], label='RMSE')
plt.plot(metrics['horizon'], metrics['mape'], label='MAPE')
plt.xlabel('Horizon')
plt.ylabel('Metrics')
plt.legend()
plt.title('Cross-Validation Performance Metrics')
plt.show()

actual_values = fb_common_features['y'].values
predicted_values = fb_forecast['yhat'].values[:-365]

fb_mae = mean_absolute_error(actual_values, predicted_values)
fb_mse = mean_squared_error(actual_values, predicted_values)
fb_rmse = np.sqrt(mse)

print(f"Fb MAE: {fb_mae}")
print(f"Fb MSE: {fb_mse}")
print(f"FB RMSE: {fb_rmse}")

